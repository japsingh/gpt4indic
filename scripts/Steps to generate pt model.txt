GPU
python .\run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\merged_tokenizer_hf" --model_type "llama" --torch_dtype "float16" --dataset_dir "C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\data" --max_train_samples 1000 --max_eval_samples 100 --streaming True --output_dir "C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\model" --do_train True --do_eval True --evaluation_strategy steps --fp16 True


CPU
python .\run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\merged_tokenizer_hf\2" --model_type "llama" --torch_dtype "float16" --dataset_dir "C:\Users\Japneet\Documents\GitHub\oscar\punjabi" --max_train_samples 1000 --max_eval_samples 100 --streaming True --output_dir "C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\model" --do_train True --do_eval True --evaluation_strategy steps --fp16 True




On vast.ai

Create A100 instance since RTX4090 gets exhausted with memory

Clone Chinese LLAMA repo:
Download text corpus in Chinese-LLaMA-Alpaca\2 folder:
Extract text corpus:

git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca.git

Copy data:
curl https://s3.amazonaws.com/datasets.huggingface.co/oscar/1.0/unshuffled/deduplicated/pa/pa_dedup.txt.gz --output ./pa_dedup.txt.gz
gunzip pa_dedup.txt.gz
split -l 51000 pa_dedup.txt

Edit requirements.txt
torch==1.13.0
peft==0.3.0
transformers==4.28.1
sentencepiece==0.1.97
datasets
scikit-learn

Execute requirements.txt
cd Chinese-LLaMA-Alpaca/
pip install -r requirements.txt

Copy this tokenizer model folder to Chinese-LLaMA-Alpaca\5  folder:
C:\Users\Japneet\Documents\GitHub\gpt4indic\punjabi\merged_tokenizer_hf\5

Move pa_dedup.txt to Chinese-LLaMA-Alpaca\2 folder
sed -i '/^$/d' pa_dedup.txt

Execute script to generate pt model: 
 
 python ./scripts/run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./5" --model_type "llama" --torch_dtype "float16" --dataset_dir "./d" --output_dir "./model" --do_train True --do_eval True --evaluation_strategy steps --dataloader_pin_memory False --gradient_checkpointing True --per_device_train_batch_size 4 --max_train_samples 100 --max_eval_samples 10 



These were tried for distributed training on multiple GPUs
Using torch.distributed.launch:
 python -m torch.distributed.launch ./scripts/run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./5" --model_type "llama" --torch_dtype "float16" --dataset_dir "./5" --max_train_samples 1000 --max_eval_samples 100 --streaming True --output_dir "./model" --do_train --do_eval --evaluation_strategy steps --fp16 True  --per_device_train_batch_size 4 --dataloader_pin_memory False --gradient_checkpointing True

 python -m torch.distributed.launch ./scripts/run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./5" --model_type "llama" --torch_dtype "float16" --dataset_dir "./d" --output_dir "./model" --do_train True --do_eval True --evaluation_strategy steps --dataloader_pin_memory False --gradient_checkpointing True --per_device_train_batch_size 4
 
Using torchrun:
 torchrun ./scripts/run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./2" --model_type "llama" --torch_dtype "float16" --dataset_dir "./2" --streaming True --output_dir "./model" --do_train True --do_eval True --evaluation_strategy steps --fp16 True --local_rank 8  --dataloader_pin_memory False --gradient_checkpointing True --auto_find_batch_size True
 
 torchrun ./scripts/run_clm_pt_with_peft.py --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./5" --model_type "llama" --torch_dtype "float16" --dataset_dir "./5" --streaming True --output_dir "./model" --do_train True --do_eval True --evaluation_strategy steps --fp16 True --dataloader_pin_memory False --gradient_checkpointing True --per_device_train_batch_size 4

  torchrun ./scripts/run_clm_pt_with_peft.py  --model_name_or_path "decapoda-research/llama-7b-hf" --tokenizer_name_or_path "./5" --model_type "llama" --torch_dtype "float16" --dataset_dir "./d" --output_dir "./model" --do_train True --do_eval True --evaluation_strategy steps --dataloader_pin_memory False --gradient_checkpointing True --per_device_train_batch_size 4




ZIP the model directory:
tar -zcvf model.tar.gz model